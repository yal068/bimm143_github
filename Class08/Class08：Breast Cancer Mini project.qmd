---
title: "Class8: Breast Cancer Mini project"
author: "Yang Liu"
format: pdf
editor: visual
---

## About

In today's lab we will work with fine needle aspiration (FNA) of breast mass data from the University of Wisconsin.

##Data Import

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df)
```

> Q. How many patients/individuals/samples are in the dataset?

```{r}
dim(wisc.df) 
nrow(wisc.df)
ncol(wisc.df)
```

> Q. How many of the observations have a malignant diagnosis?

```{r}
sum(wisc.df$diagnosis == "M")
```

```{r}
table(wisc.df$diagnosis)
```

```{r}
colnames(wisc.df)
```

> Q. How many variables/features in the data are suffixed with \_mean?

```{r}
inds <- grep("_mean", colnames(wisc.df))
length(inds)
```

```{r}
grep("_mean", colnames(wisc.df), value = T)
```

##Initial Analysis

Before analysis, we want to take out the export data diagnosis column (a.k.a. the answer) from our dataset

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
head(diagnosis)
```

```{r}
wisc.data <- wisc.df[,-1]
```

##Clustering

We can try a kmeans() clustering first

```{r}
km <- kmeans(wisc.data, centers = 2)
km$cluster
table(km$cluster)
```

Cross-table

```{r}
table(km$cluster, diagnosis)
```

Let's try 'hclust()' the key input required for 'hclust()' us a distance matrix as produced by the "dist()" function

```{r}
hc <- hclust(dist(wisc.data))
plot(hc)
```

##PCA

Do we need to scale the data?

we can look at sd first

```{r}
apply(wisc.data, 2, sd)
```

yes, we need to scale

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

```{r}
library(ggplot2)

res <- as.data.frame(wisc.pr$x)
```

```{r}
ggplot(res) + 
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

```{r}
ggplot(res) + 
  aes(PC1, PC3, col=diagnosis) +
  geom_point()
```

Purpose of PCA: PCA takes a dataset with a lot of dimensions

##clustering methods Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward's criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

Clustering on PCA results

```{r}
d <- dist(wisc.pr$x[, 1:3])
hc <- hclust(d, method="ward.D2")
plot(hc)
```

To get my clustering result/membership vector, I need to "cut" the tree with the "cutree()" function.

```{r}
grps <- cutree(hc, k=2)
```

> Q. How many patients are in each cluster group?

```{r}
table(grps)
```

```{r}
plot(res$PC1, res$PC2, col=grps)
```

## Prediction

We can use our PCA result (model) to do prediction, that is take new unseen data and project it onto our new PC varaibles.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```


```{r}
plot(res$PC1, res$PC2, col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], labels=c(1, 2), col="white")
```

#Summary

Principle Component Analysis (PCA) is a super useful method for analyzing large datasets. It works by finding new variables (PCs) that capture the most variance from the original variable in your dataset.